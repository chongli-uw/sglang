+ python -m sglang.bench_serving --num-prompts 1024 --dataset-name dapo --dapo-max-resp-len 512 --disable-stream
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=None, dataset_name='dapo', dataset_path='', model=None, tokenizer=None, num_prompts=1024, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='dapo', dataset_path='', model='/fsx/checkpoints/DeepSeek-V2', tokenizer=None, num_prompts=1024, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 154843
#Output tokens: 524288
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...
  0%|          | 0/1024 [00:00<?, ?it/s]  0%|          | 1/1024 [01:44<29:46:35, 104.79s/it]  0%|          | 2/1024 [01:44<12:16:59, 43.27s/it]  38%|███▊      | 385/1024 [01:45<01:25,  7.43it/s]  88%|████████▊ | 901/1024 [01:45<00:05, 21.64it/s]100%|██████████| 1024/1024 [01:45<00:00,  9.72it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    inf       
Max request concurrency:                 not set   
Successful requests:                     1024      
Benchmark duration (s):                  105.31    
Total input tokens:                      154843    
Total generated tokens:                  524288    
Total generated tokens (retokenized):    521793    
Request throughput (req/s):              9.72      
Input token throughput (tok/s):          1470.29   
Output token throughput (tok/s):         4978.29   
Total token throughput (tok/s):          6448.58   
Concurrency:                             1021.48   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   105055.69 
Median E2E Latency (ms):                 105071.32 
---------------Time to First Token----------------
Mean TTFT (ms):                          105055.70 
Median TTFT (ms):                        105071.33 
P99 TTFT (ms):                           105278.07 
---------------Inter-Token Latency----------------
Mean ITL (ms):                           0.00      
Median ITL (ms):                         0.00      
P95 ITL (ms):                            0.00      
P99 ITL (ms):                            0.00      
Max ITL (ms):                            0.00      
==================================================
+ python -m sglang.bench_serving --num-prompts 2048 --dataset-name dapo --dapo-max-resp-len 512 --disable-stream
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=None, dataset_name='dapo', dataset_path='', model=None, tokenizer=None, num_prompts=2048, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='dapo', dataset_path='', model='/fsx/checkpoints/DeepSeek-V2', tokenizer=None, num_prompts=2048, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 311102
#Output tokens: 1048576
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...
  0%|          | 0/2048 [00:00<?, ?it/s]  0%|          | 1/2048 [01:42<58:00:43, 102.02s/it]  0%|          | 5/2048 [01:42<8:39:07, 15.25s/it]   26%|██▌       | 534/2048 [01:42<02:22, 10.59it/s] 51%|█████     | 1046/2048 [01:42<00:39, 25.07it/s] 76%|███████▌  | 1555/2048 [01:42<00:10, 45.49it/s]100%|██████████| 2048/2048 [01:42<00:00, 19.95it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    inf       
Max request concurrency:                 not set   
Successful requests:                     2048      
Benchmark duration (s):                  102.67    
Total input tokens:                      311102    
Total generated tokens:                  1048576   
Total generated tokens (retokenized):    1043501   
Request throughput (req/s):              19.95     
Input token throughput (tok/s):          3030.08   
Output token throughput (tok/s):         10212.94  
Total token throughput (tok/s):          13243.02  
Concurrency:                             2039.50   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   102245.02 
Median E2E Latency (ms):                 102244.50 
---------------Time to First Token----------------
Mean TTFT (ms):                          102245.03 
Median TTFT (ms):                        102244.51 
P99 TTFT (ms):                           102601.01 
---------------Inter-Token Latency----------------
Mean ITL (ms):                           0.00      
Median ITL (ms):                         0.00      
P95 ITL (ms):                            0.00      
P99 ITL (ms):                            0.00      
Max ITL (ms):                            0.00      
==================================================
+ python -m sglang.bench_serving --num-prompts 4096 --dataset-name dapo --dapo-max-resp-len 512 --disable-stream
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=None, dataset_name='dapo', dataset_path='', model=None, tokenizer=None, num_prompts=4096, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='dapo', dataset_path='', model='/fsx/checkpoints/DeepSeek-V2', tokenizer=None, num_prompts=4096, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 623293
#Output tokens: 2097152
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...
  0%|          | 0/4096 [00:00<?, ?it/s]  0%|          | 1/4096 [01:48<123:15:34, 108.36s/it]  0%|          | 2/4096 [01:48<50:52:09, 44.73s/it]    0%|          | 16/4096 [01:48<3:58:48,  3.51s/it] 13%|█▎        | 515/4096 [01:48<04:24, 13.54it/s]  19%|█▉        | 770/4096 [01:49<02:23, 23.11it/s] 56%|█████▋    | 2306/4096 [01:49<00:16, 105.37it/s] 83%|████████▎ | 3408/4096 [01:49<00:03, 188.33it/s]100%|██████████| 4096/4096 [01:49<00:00, 37.44it/s] 

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    inf       
Max request concurrency:                 not set   
Successful requests:                     4096      
Benchmark duration (s):                  109.42    
Total input tokens:                      623293    
Total generated tokens:                  2097152   
Total generated tokens (retokenized):    2086919   
Request throughput (req/s):              37.43     
Input token throughput (tok/s):          5696.18   
Output token throughput (tok/s):         19165.57  
Total token throughput (tok/s):          24861.75  
Concurrency:                             4067.67   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   108666.13 
Median E2E Latency (ms):                 108695.30 
---------------Time to First Token----------------
Mean TTFT (ms):                          108666.14 
Median TTFT (ms):                        108695.31 
P99 TTFT (ms):                           109298.08 
---------------Inter-Token Latency----------------
Mean ITL (ms):                           0.00      
Median ITL (ms):                         0.00      
P95 ITL (ms):                            0.00      
P99 ITL (ms):                            0.00      
Max ITL (ms):                            0.00      
==================================================
+ python -m sglang.bench_serving --num-prompts 8192 --dataset-name dapo --dapo-max-resp-len 512 --disable-stream
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=None, dataset_name='dapo', dataset_path='', model=None, tokenizer=None, num_prompts=8192, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=30000, dataset_name='dapo', dataset_path='', model='/fsx/checkpoints/DeepSeek-V2', tokenizer=None, num_prompts=8192, num_responses=1, sharegpt_output_len=None, sharegpt_context_len=None, dapo_max_resp_len=512, dapo_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=inf, max_concurrency=None, output_file=None, output_details=False, disable_tqdm=False, disable_stream=True, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 1243825
#Output tokens: 4194304
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...
  0%|          | 0/8192 [00:00<?, ?it/s]  0%|          | 1/8192 [02:34<351:47:34, 154.62s/it]  0%|          | 3/8192 [02:34<91:24:09, 40.18s/it]    0%|          | 4/8192 [02:35<59:53:38, 26.33s/it]  1%|          | 82/8192 [02:35<1:30:52,  1.49it/s]  6%|▋         | 515/8192 [02:35<09:52, 12.95it/s]  13%|█▎        | 1026/8192 [02:35<03:43, 32.05it/s] 44%|████▍     | 3586/8192 [02:36<00:27, 167.52it/s] 62%|██████▏   | 5041/8192 [02:36<00:11, 275.71it/s] 72%|███████▏  | 5887/8192 [02:36<00:06, 362.49it/s] 82%|████████▏ | 6717/8192 [02:36<00:03, 446.91it/s] 89%|████████▉ | 7330/8192 [02:37<00:01, 554.47it/s] 96%|█████████▋| 7902/8192 [02:37<00:00, 687.74it/s]100%|██████████| 8192/8192 [02:37<00:00, 52.13it/s] 

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    inf       
Max request concurrency:                 not set   
Successful requests:                     8192      
Benchmark duration (s):                  157.17    
Total input tokens:                      1243825   
Total generated tokens:                  4194304   
Total generated tokens (retokenized):    4173681   
Request throughput (req/s):              52.12     
Input token throughput (tok/s):          7913.66   
Output token throughput (tok/s):         26685.66  
Total token throughput (tok/s):          34599.32  
Concurrency:                             8083.17   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   155086.36 
Median E2E Latency (ms):                 155135.94 
---------------Time to First Token----------------
Mean TTFT (ms):                          155086.37 
Median TTFT (ms):                        155135.95 
P99 TTFT (ms):                           156906.44 
---------------Inter-Token Latency----------------
Mean ITL (ms):                           0.00      
Median ITL (ms):                         0.00      
P95 ITL (ms):                            0.00      
P99 ITL (ms):                            0.00      
Max ITL (ms):                            0.00      
==================================================
